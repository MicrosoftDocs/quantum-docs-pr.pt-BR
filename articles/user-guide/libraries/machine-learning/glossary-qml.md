---
title: Biblioteca de aprendizado de máquina quântico
author: alexeib2
ms.author: alexei.bocharov@microsoft.com
ms.date: 2/27/2020
ms.topic: article
uid: microsoft.quantum.libraries.machine-learning.training
ms.openlocfilehash: f9b33a607a892179795d0700ba3080f9a24ab94a
ms.sourcegitcommit: 0181e7c9e98f9af30ea32d3cd8e7e5e30257a4dc
ms.translationtype: MT
ms.contentlocale: pt-BR
ms.lasthandoff: 06/23/2020
ms.locfileid: "85274340"
---
# <a name="quantum-machine-learning-glossary"></a><span data-ttu-id="ca0c9-102">Glossário de Machine Learning Quantum</span><span class="sxs-lookup"><span data-stu-id="ca0c9-102">Quantum Machine Learning glossary</span></span>

<span data-ttu-id="ca0c9-103">O treinamento de um classificador de Quantum centrado em circuito é um processo com muitas partes móveis que exigem a mesma quantidade (ou um pouco maior) de calibragem por avaliação e erro como treinamento de classificadores tradicionais.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-103">Training of a circuit-centric quantum classifier is a process with many moving parts that require the same (or slightly larger) amount of calibration by trial and error as training of traditional classifiers.</span></span> <span data-ttu-id="ca0c9-104">Aqui, definimos os principais conceitos e ingredientes deste processo de treinamento.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-104">Here we define the main concepts and ingredients of this training process.</span></span>

## <a name="trainingtesting-schedules"></a><span data-ttu-id="ca0c9-105">Agendamentos/testes de treinamento</span><span class="sxs-lookup"><span data-stu-id="ca0c9-105">Training/testing schedules</span></span>

<span data-ttu-id="ca0c9-106">No contexto do treinamento do classificador, uma *agenda* descreve um subconjunto de exemplos de dados em um conjunto de treinamento ou teste geral.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-106">In the context of classifier training a *schedule* describes a subset of data samples in an overall training or testing set.</span></span> <span data-ttu-id="ca0c9-107">Uma agenda geralmente é definida como uma coleção de índices de exemplo.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-107">A schedule is usually defined as a collection of sample indices.</span></span>

## <a name="parameterbias-scores"></a><span data-ttu-id="ca0c9-108">Resultados de parâmetro/diferença</span><span class="sxs-lookup"><span data-stu-id="ca0c9-108">Parameter/bias scores</span></span>

<span data-ttu-id="ca0c9-109">Dado um vetor de parâmetro candidato e uma diferença de classificador, sua *Pontuação de validação* é medida em relação a uma agenda de validação escolhida S e é expressa por um número de classificações incorretas contadas em todos os exemplos na agenda s.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-109">Given a candidate parameter vector and a classifier bias, their *validation score* is measured relative to a chosen validation schedule S and is expressed by a number of misclassifications counted over all the samples in the schedule S.</span></span>

## <a name="hyperparameters"></a><span data-ttu-id="ca0c9-110">Hiperparâmetros</span><span class="sxs-lookup"><span data-stu-id="ca0c9-110">Hyperparameters</span></span>

<span data-ttu-id="ca0c9-111">O processo de treinamento do modelo é regido por determinados valores predefinidos chamados *hiperparâmetros*:</span><span class="sxs-lookup"><span data-stu-id="ca0c9-111">The model training process is governed by certain pre-set values called *hyperparameters*:</span></span>

### <a name="learning-rate"></a><span data-ttu-id="ca0c9-112">Taxa de aprendizado</span><span class="sxs-lookup"><span data-stu-id="ca0c9-112">Learning rate</span></span>

<span data-ttu-id="ca0c9-113">É um dos hiperparâmetros de chave.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-113">It is one of the key hyperparameters.</span></span> <span data-ttu-id="ca0c9-114">Ele define a quantidade atual de estimativas de gradiente estocástico impacta a atualização do parâmetro.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-114">It defines how much current stochastic gradient estimate impacts the parameter update.</span></span> <span data-ttu-id="ca0c9-115">O tamanho do Delta de atualização de parâmetro é proporcional à taxa de aprendizado.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-115">The size of parameter update delta is proportional to the learning rate.</span></span> <span data-ttu-id="ca0c9-116">Os valores menores de taxa de aprendizagem levam à redução mais lenta de parâmetros e à convergência mais lenta, mas valores excessivamente grandes de LR podem quebrar a convergência completamente, pois o gradiente descendente nunca se compromete com um mínimo local específico.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-116">Smaller learning rate values lead to slower parameter evolution and slower convergence, but excessively large values of LR may break the convergence altogether as the gradient descent never commits to a particular local minimum.</span></span> <span data-ttu-id="ca0c9-117">Embora a taxa de aprendizado seja ajustada adaptativamente pelo algoritmo de treinamento até certo ponto, é importante selecionar um bom valor inicial para ele.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-117">While learning rate is adaptively adjusted by the training algorithm to some extent, selecting a good initial value for it is important.</span></span> <span data-ttu-id="ca0c9-118">Um valor inicial padrão comum para a taxa de aprendizado é 0,1.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-118">A usual default initial value for learning rate is 0.1.</span></span> <span data-ttu-id="ca0c9-119">A seleção do melhor valor da taxa de aprendizado é uma arte fina (consulte, por exemplo, a seção 4,3 de Goodfellow et al., "aprendizado profundo", MIT Press, 2017).</span><span class="sxs-lookup"><span data-stu-id="ca0c9-119">Selecting the best value of learning rate is a fine art (see, for example, section 4.3 of Goodfellow et al.,"Deep learning", MIT Press, 2017).</span></span>

### <a name="minibatch-size"></a><span data-ttu-id="ca0c9-120">Tamanho do minilote</span><span class="sxs-lookup"><span data-stu-id="ca0c9-120">Minibatch size</span></span>

<span data-ttu-id="ca0c9-121">Define quantas amostras de dados são usadas para uma única estimativa de gradiente estocástico.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-121">Defines how many data samples is used for a single estimation of stochastic gradient.</span></span> <span data-ttu-id="ca0c9-122">Valores maiores de tamanho de minilote geralmente levam a uma convergência mais robusta e monotônico, mas podem retardar o processo de treinamento, pois o custo de qualquer estimativa de gradiente é proporcional ao tamanho do minimatch.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-122">Larger values of minibatch size generally lead to more robust and more monotonic convergence but can potentially slow down the training process, as the cost of any one gradient estimation is proportional to the minimatch size.</span></span> <span data-ttu-id="ca0c9-123">Um valor padrão usual para o tamanho de minilote é 10.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-123">A usual default value for the minibatch size is 10.</span></span>

### <a name="training-epochs-tolerance-gridlocks"></a><span data-ttu-id="ca0c9-124">Épocas de treinamento, tolerância, gridlocks</span><span class="sxs-lookup"><span data-stu-id="ca0c9-124">Training epochs, tolerance, gridlocks</span></span>

<span data-ttu-id="ca0c9-125">"Época" significa uma passagem completa dos dados de treinamento agendados.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-125">"Epoch" means one complete pass through the scheduled training data.</span></span>
<span data-ttu-id="ca0c9-126">O número máximo de épocas por um thread de treinamento (veja abaixo) deve ser limitado.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-126">The maximum number of epochs per a training thread (see below) should be capped.</span></span> <span data-ttu-id="ca0c9-127">O thread de treinamento é definido para terminar (com os melhores parâmetros candidatos conhecidos) quando o número máximo de épocas é executado.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-127">The training thread is defined to terminate (with the best known candidate parameters) when the maximum number of epochs has been executed.</span></span> <span data-ttu-id="ca0c9-128">No entanto, esse treinamento será encerrado anteriormente quando a taxa de classificação insignificante na agenda de validação cair abaixo de uma tolerância escolhida.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-128">However such training would terminate earlier when misclassification rate on validation schedule falls below a chosen tolerance.</span></span> <span data-ttu-id="ca0c9-129">Suponha, por exemplo, que a tolerância de classificação incorreta seja 0, 1 (1%); se estiver no conjunto de validação de exemplos de 2000, estamos vendo menos de 20 classificações incorretas, o nível de tolerância foi atingido.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-129">Suppose, for example, that misclassification tolerance is 0.01 (1%); if on validation set of 2000 samples we are seeing fewer than 20 misclassifications, then the tolerance level has been achieved.</span></span> <span data-ttu-id="ca0c9-130">Um thread de treinamento também será encerrado prematuramente se a pontuação de validação do modelo candidato não tiver mostrado nenhuma melhoria em várias épocas consecutivas (um engarrafamentos).</span><span class="sxs-lookup"><span data-stu-id="ca0c9-130">A training thread also terminates prematurely if the validation score of the candidate model has not shown any improvement over several consecutive epochs (a gridlock).</span></span> <span data-ttu-id="ca0c9-131">A lógica para o encerramento de engarrafamentos é codificada no momento.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-131">The logic for the gridlock termination is currently hardcoded.</span></span>

### <a name="measurements-count"></a><span data-ttu-id="ca0c9-132">Contagem de medidas</span><span class="sxs-lookup"><span data-stu-id="ca0c9-132">Measurements count</span></span>

<span data-ttu-id="ca0c9-133">Estimar as pontuações de treinamento/validação e os componentes do gradiente estocástico em valores de dispositivos Quantum para estimar sobreposições de estado de Quantum que exigem várias medidas das observáveis apropriadas.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-133">Estimating the training/validation scores and the components of the stochastic gradient on a quantum device amounts to estimating quantum state overlaps that requires multiple measurements of the appropriate observables.</span></span> <span data-ttu-id="ca0c9-134">O número de medições deve ser dimensionado como $O (1/\ Épsilon ^ 2) $, em que $ \epsilon $ é o erro de estimativa desejado.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-134">The number of measurements should scale as $O(1/\epsilon^2)$ where $\epsilon$ is the desired estimation error.</span></span>
<span data-ttu-id="ca0c9-135">Como regra geral, a contagem de medidas inicial pode ser de aproximadamente US $1/\ mbox {tolerância} ^ 2 $ (consulte definição de tolerância no parágrafo anterior).</span><span class="sxs-lookup"><span data-stu-id="ca0c9-135">As a rule of thumb, the initial measurements count could be approximately $1/\mbox{tolerance}^2$ (see definition of tolerance in the previous paragraph).</span></span> <span data-ttu-id="ca0c9-136">Uma delas precisaria revisar a contagem de medidas para cima se o gradiente descendente parecer estar muito irregular e a convergência muito difícil de ser atingida.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-136">One would need to revise the measurement count upward if the gradient descent appears to be too erratic and convergence too hard to achieve.</span></span>

### <a name="training-threads"></a><span data-ttu-id="ca0c9-137">Threads de treinamento</span><span class="sxs-lookup"><span data-stu-id="ca0c9-137">Training threads</span></span>

<span data-ttu-id="ca0c9-138">A função de probabilidade, que é o utilitário de treinamento para o classificador, raramente é convexa, o que significa que ele geralmente tem uma infinidade de optima locais no espaço de parâmetro que pode diferir significativamente por qualidade.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-138">The likelihood function which is the training utility for the classifier is very seldom convex, meaning that it usually has a multitude of local optima in the parameter space that may differ significantly by quality.</span></span> <span data-ttu-id="ca0c9-139">Como o processo SGD pode convergir para apenas um ideal, é importante explorar vários vetores de parâmetro de início.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-139">Since the SGD process can converge to only one specific optimum, it is important to explore multiple starting parameter vectors.</span></span> <span data-ttu-id="ca0c9-140">A prática comum no aprendizado de máquina é inicializar aleatoriamente esses vetores iniciais.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-140">Common practice in machine learning is to initialize such starting vectors randomly.</span></span> <span data-ttu-id="ca0c9-141">A API de treinamento do Q # aceita uma matriz arbitrária de tais vetores de início, mas o código subjacente os explora em sequência.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-141">The Q# training API accepts an arbitrary array of such starting vectors but the underlying code explores them sequentially.</span></span> <span data-ttu-id="ca0c9-142">Em um computador de vários núcleos ou na verdade, em qualquer arquitetura de computação paralela, é aconselhável executar várias chamadas para a API de treinamento Q # em paralelo com inicializações de parâmetro diferentes nas chamadas.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-142">On a multicore computer or in fact on any parallel computing architecture it is advisable to perform several calls to Q# training API in parallel with different parameter initializations across the calls.</span></span>

#### <a name="how-to-modify-the-hyperparameters"></a><span data-ttu-id="ca0c9-143">Como modificar os hiperparâmetros</span><span class="sxs-lookup"><span data-stu-id="ca0c9-143">How to modify the hyperparameters</span></span>

<span data-ttu-id="ca0c9-144">Na biblioteca QML, a melhor maneira de modificar os hiperparâmetros é substituindo os valores padrão de UDT [`TrainingOptions`](xref:microsoft.quantum.machinelearning.trainingoptions) .</span><span class="sxs-lookup"><span data-stu-id="ca0c9-144">In the QML library, the best way to modify the hyperparameters is by overriding the default values of the UDT [`TrainingOptions`](xref:microsoft.quantum.machinelearning.trainingoptions).</span></span> <span data-ttu-id="ca0c9-145">Para fazer isso, chamamos isso de função [`DefaultTrainingOptions`](xref:microsoft.quantum.machinelearning.defaulttrainingoptions) e aplicamos o operador `w/` para substituir os valores padrão.</span><span class="sxs-lookup"><span data-stu-id="ca0c9-145">To do this we call it with the function [`DefaultTrainingOptions`](xref:microsoft.quantum.machinelearning.defaulttrainingoptions) and apply the operator `w/` to override the default values.</span></span> <span data-ttu-id="ca0c9-146">Por exemplo, para usar medidas de 100.000 e uma taxa de aprendizagem de 0, 1:</span><span class="sxs-lookup"><span data-stu-id="ca0c9-146">For example, to use 100,000 measurements and a learning rate of 0.01:</span></span>
 ```qsharp
let options = DefaultTrainingOptions()
w/ LearningRate <- 0.01
w/ NMeasurements <- 100000;
 ```
